{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizer\n",
    "import torch.utils.data as Data\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"jack like dog\", \"jack like cat\", \"jack like animal\",\n",
    "  \"dog cat animal\", \"banana apple cat dog like\", \"dog fish milk like\",\n",
    "  \"dog cat animal like\", \"jack like apple\", \"apple like\", \"jack like banana\",\n",
    "  \"apple banana jack movie book music like\", \"cat dog hate\", \"cat dog like\"]\n",
    "\n",
    "sentences_list = \" \".join(sentences).split()  # ['jack', 'like', 'dog']\n",
    "vocab = list(set(sentences_list))  #  构建词汇表\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameter\n",
    "window_size = 2\n",
    "batch_size = 8\n",
    "m = 2  #  每个词用2维表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = []\n",
    "for idx in range(window_size, len(sentences_list) - window_size):\n",
    "    center = word2idx[sentences_list[idx]]\n",
    "    word_background_list = sentences_list[idx - window_size: idx] + sentences_list[idx + 1: idx + window_size + 1]\n",
    "    context = [word2idx[background_word] for background_word in word_background_list]\n",
    "\n",
    "    for w in context:\n",
    "        skip_grams.append([center, w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(skip_grams):\n",
    "    input_data = []\n",
    "    output_data = []\n",
    "    for a, b in skip_grams:\n",
    "        input_data.append(np.eye(vocab_size)[a])\n",
    "        output_data.append(b)\n",
    "    return input_data, output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, output_data = make_data(skip_grams)\n",
    "input_data, output_data = torch.Tensor(input_data), torch.LongTensor(output_data)\n",
    "dataset = Data.TensorDataset(input_data, output_data)\n",
    "loader = Data.DataLoader(dataset, batch_size, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(vocab_size, m).type(dtype))\n",
    "        self.V = nn.Parameter(torch.randn(m, vocab_size).type(dtype))\n",
    "\n",
    "    def forward(self, X):\n",
    "        #  X : [batch_size, vocab_size]\n",
    "        hidden = torch.mm(X, self.W) #  [batch_size, m]\n",
    "        output = torch.mm(hidden, self.V) #  [batch_size, vocab_size]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optim = optimizer.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0 1.9726399183273315\n",
      "1000 1 2.087526798248291\n",
      "1000 2 2.381875514984131\n",
      "1000 3 2.397819757461548\n",
      "1000 4 1.9717426300048828\n",
      "1000 5 2.1237902641296387\n",
      "1000 6 1.9658904075622559\n",
      "1000 7 2.118483543395996\n",
      "1000 8 2.0691521167755127\n",
      "1000 9 1.875338077545166\n",
      "1000 10 2.274322271347046\n",
      "1000 11 2.147645950317383\n",
      "1000 12 2.271571397781372\n",
      "1000 13 1.7344664335250854\n",
      "1000 14 2.119657039642334\n",
      "1000 15 1.4634943008422852\n",
      "1000 16 2.2701416015625\n",
      "1000 17 2.069187641143799\n",
      "1000 18 2.525500774383545\n",
      "1000 19 1.65653657913208\n",
      "1000 20 2.248680830001831\n",
      "2000 0 1.8978071212768555\n",
      "2000 1 2.14563250541687\n",
      "2000 2 2.042457342147827\n",
      "2000 3 1.661184549331665\n",
      "2000 4 2.083333730697632\n",
      "2000 5 2.1225814819335938\n",
      "2000 6 1.989145278930664\n",
      "2000 7 2.045304298400879\n",
      "2000 8 1.9921973943710327\n",
      "2000 9 2.3354737758636475\n",
      "2000 10 2.1458075046539307\n",
      "2000 11 2.134782552719116\n",
      "2000 12 2.226255178451538\n",
      "2000 13 2.664048433303833\n",
      "2000 14 2.0355443954467773\n",
      "2000 15 2.0177648067474365\n",
      "2000 16 2.258944034576416\n",
      "2000 17 1.787781834602356\n",
      "2000 18 1.8995071649551392\n",
      "2000 19 1.8599801063537598\n",
      "2000 20 2.0074357986450195\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2000):\n",
    "    for i, (batch_x, batch_y) in enumerate(loader):\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        pred = model(batch_x)\n",
    "        loss = loss_fn(pred, batch_y)\n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(epoch + 1, i, loss.item())\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()   \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "304cd1c94380144a02ff01137cb404d5f8049bb530782fa61003898634b0fbe9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('d2l-zh')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
